{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#brown corpus\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "print brown.categories()\n",
    "print brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist( (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor'] \n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will'] \n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading own corpus\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = path_file\n",
    "wordLists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordLists.words(wordLists.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bigrams\n",
    "sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n",
    "list(nltk.bigrams(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#proportion of non-stopping words\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return 1.0 * len(content) / len(text)\n",
    "\n",
    "content_fraction(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pronoucing dictionary\n",
    "entries = nltk.corpus.cmudict.entries()\n",
    "for entry in entries[39943:39951]:\n",
    "    print entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find words with similar pronounciation\n",
    "for word, pron in entries:\n",
    "    if len(pron) == 3:\n",
    "        p1, p2, p3 = pron\n",
    "        if p1 == 'P' and p3 == 'T':\n",
    "            print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "#toolbox.entries('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wordnet synonyms\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at concepts that're more specific\n",
    "wn.synset('car.n.01').hyponyms()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at concepts that're more general\n",
    "wn.synset('car.n.01').hypernym_paths()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the `include` relationship\n",
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#semantic similarity of two words\n",
    "right = wn.synset('right_whale.n.01') \n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#higher min_depth, more similar\n",
    "right.lowest_common_hypernyms(minke)[0].min_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "from urllib import urlopen\n",
    "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "raw = urlopen(url).read()\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokens to nltk text\n",
    "text = nltk.Text(tokens)\n",
    "text.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words commonly appear together\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence segmentation\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = nltk.corpus.gutenberg.raw('some_file')\n",
    "sens = sent_tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The text.similar() method takes a word w, finds all contexts w1w w2, then finds all words w' that appear in the same context, i.e. w1w'w2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.similar(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagged corpus\n",
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#what POS happen before a noun\n",
    "word_tag_pairs = nltk.bigrams(brown_news_tagged)\n",
    "list(nltk.FreqDist(a[1] for (a, b) in word_tag_pairs if b[1].startswith('N')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Automatic tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regular expression tagger\n",
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),\n",
    "    (r'.*ed$', 'VBD'),\n",
    "    (r'.*es$', 'VBZ'),\n",
    "    (r'.*ould$', 'MD'),\n",
    "    (r'.*\\'s$', 'NN$'),\n",
    "    (r'.*s$', 'NNS'),\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "    (r'.*', 'NN') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first one that matches is applied\n",
    "regex_tagger = nltk.RegexpTagger(patterns)\n",
    "regex_tagger.tag(brown.sents(categories='news')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look up tagger\n",
    "#use the most frequent words and their most frequent tag\n",
    "fd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "most_freq_words = fd.keys()[:100]\n",
    "likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "\n",
    "sent = brown.sents(categories='news')[3]\n",
    "baseline_tagger.tag(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using backoff\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
    "                                         backoff=nltk.DefaultTagger('NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_tagger.evaluate(brown.tagged_sents(categories='news'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uni-grame tagger\n",
    "#same as look up\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n-gram tagger\n",
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "bigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> N-gram taggers should not consider context that crosses a sentence boundary. Accordingly, NLTK taggers are designed to work with lists of sentences, where each sentence is a list of words. At the start of a sentence, tn-1 and preceding tags are set to None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Taggers   \n",
    "\n",
    "1. Try tagging the token with the bigram tagger.\n",
    "2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger. \n",
    "3. If the unigram tagger is also unable to find a tag, use a default tagger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(brown_tagged_sents, backoff=t0)\n",
    "#cut off specify the pattern has to appear over 2 times to be used\n",
    "t2 = nltk.BigramTagger(brown_tagged_sents, cutoff=2, backoff=t1)\n",
    "\n",
    "t2.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging unknow words  \n",
    "> replacing unknownn words with UNK, so that the n-gram can learn the pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brill Tagging\n",
    "> The general idea is very simple: guess the tag of each word, then go back and fix the mistakes. In this way, a Brill tagger successively transforms a bad tagging of a text into a better one. As with n-gram tagging, this is a supervised learning method, since we need an- notated training data to figure out whether the tagger’s guess is a mistake or not. How- ever, unlike n-gram tagging, it does not count observations but compiles a list of trans- formational correction rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s look at an example in- volving the following sentence:\n",
    "__The President said he will ask Congress to increase grants to states for voca- tional rehabilitation.__\n",
    "> We will examine the operation of two rules: (a) replace NN with VB when the previous word is TO; (b) replace TO with IN when the next tag is NNS. Table 5-6 illustrates this process, first tagging with the unigram tagger, then applying the rules to fix the errors.\n",
    "\n",
    "![Img](./image/Brill.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.tag.brill as brill\n",
    "\n",
    "\"Source code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f219afaf860f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#sentence segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebank_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mboundary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#sentence segmentation\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "\n",
    "boundary = np.array([len(x) for x in sents])-1\n",
    "\n",
    "boundary = np.cumsum(boundary)\n",
    "\n",
    "sents = [x for y in sents for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build up feature set\n",
    "features = []\n",
    "for i in range(1, len(sents)-1):\n",
    "    if sents[i] not in '.?!':\n",
    "        continue\n",
    "    next_word_cap = sents[i+1][0].isupper()\n",
    "    prevword = sents[i-1].lower()\n",
    "    prev_word_one_char = len(sents[i-1]) == 1\n",
    "    is_bountry = i in sents\n",
    "    features.append((next_word_cap, prevword, prev_word_one_char, is_bountry))\n",
    "\n",
    "#then run some classification like naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![information retrieval](./image/info_retrieval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "\n",
    "def ie_process(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Noun Phrase Chunking\n",
    "\n",
    "> In this case, we will define a simple grammar with a single regular expression rule . This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). Using this grammar, we create a chunk parser , and test it on our example sentence . The result is a tree, which we can either print , or display graphically ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.Tree'>\n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar  = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result  = cp.parse(sentence)\n",
    "print type(result)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and nouns\n",
    "{<NNP>+} # chunk sequences of proper nouns \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "(\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IOB tags\n",
    "> In this scheme, each token is tagged with one of three special chunk tags, I (inside), O (outside), or B (begin). A token is tagged as B if it marks the beginning of a chunk. Subsequent tokens within the chunk are tagged I. All other tokens are tagged O.\n",
    "\n",
    "![iob](./image/IOB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing and Evaluating Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [(u'he', u'PRP')]), (u'accepted', u'VBD'), Tree('NP', [(u'the', u'DT')])])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading IOB format and converting to nltk tree\n",
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "'''\n",
    "\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP'])\n",
    "#if come across Lookup Error; Install ghostdriver with:\n",
    "#brew install ghostdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#CoNLL-2000 chunking corpus\n",
    "from nltk.corpus import conll2000\n",
    "print conll2000.chunked_sents('train.txt')[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline: no chunks\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "len(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%\n",
      "    Precision:      0.0%\n",
      "    Recall:         0.0%\n",
      "    F-Measure:      0.0%\n"
     ]
    }
   ],
   "source": [
    "print cp.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%\n",
      "    Precision:     70.6%\n",
      "    Recall:        67.8%\n",
      "    F-Measure:     69.2%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print cp.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using unigram tagger to build chunker\n",
    "#from POS-tag to chunk\n",
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                in zip(sentence, chunktags)]\n",
    "        #note all the tags is in list of tags, so use colltags2tree not collstr2tree\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%\n",
      "    Precision:     79.9%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     83.2%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print unigram_chunker.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                in zip(sentence, chunktags)]\n",
    "        #note all the tags is in list of tags, so use colltags2tree not collstr2tree\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%\n",
      "    Precision:     82.3%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     84.5%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print bigram_chunker.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier-Based Chunkers\n",
    "a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
    "b. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
    "\n",
    "> These two sentences have the same part-of-speech tags, yet they are chunked differ- ently. In the first sentence, the farmer and rice are separate chunks, while the corre- sponding material in the second sentence, the computer monitor, is a single chunk. Clearly, we need to make use of information about the content of the words, in addition to just their part-of-speech tags, if we wish to maximize chunking performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConsecutiveNpChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "            self.classifier = nltk.MaxentClassifier.train(train_set, algorithm='megam', trace=0)\n",
    "            \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classifier(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "\n",
    "class ConsecutiveNpChunk(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3958a4b82b3d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3958a4b82b3d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    class ConsecutiveNPChunkTagger()\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#traning classifier_based chunkers\n",
    "\n",
    "#simple feature extraction:\n",
    "#  just provide the POS tag of the current token (like unigram)\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\":pos}\n",
    "\n",
    "\n",
    "#add previous POS tag:\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    return {\"pos\":pos, \"prevpos\":prevpos}\n",
    "\n",
    "\n",
    "#also adding the current word\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    return {\"pos\":pos, \"prevpos\":prevpos, \"word\":word}\n",
    "\n",
    "#we can also adding other features like next POS, (previous POS, current POS)\n",
    "#detail on Page 277\n",
    "\n",
    "\n",
    "import nltk\n",
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "            self.classifier = nltk.MaxentClassifier.train(train_set, algorithm=='megam', trace=0)\n",
    "        \n",
    "        def tag(self, sentences):\n",
    "            history = []\n",
    "            for i, word in enumerate(sentences):\n",
    "                featureset = npchunk_features(sentence, i, history)\n",
    "                tag = self.classifier.classify(featureset)\n",
    "                history.append(tag)\n",
    "            return zip(sentence, history)\n",
    "    \n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in nltk.chunk.tree2conlltags(sent)]\n",
    "                for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "        \n",
    "    def parser(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w, t, c) for ((w, t), c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recursion in Linguistic Structure\n",
    "### Building Nested Structure with Cascaded Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "#one rule per line in the grammar\n",
    "grammar = r\"\"\"\n",
    "NP: {<DT|JJ|NN.*>+}\n",
    "PP: {<IN><NP>}\n",
    "VP: {<VB.*><NP|PP|CLAUSE>+$}\n",
    "CLAUSE: {<NP><VP>}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Unfortunately this result misses the VP headed by saw. \n",
    "> The solution to these problems is to get the chunker to loop over its patterns: after trying all of them, it repeats the process. We add an optional second argument loop to specify the number of times the set of patterns should be run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop=2)\n",
    "print cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees\n",
    "![Tree](./image/Tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Alice)\n"
     ]
    }
   ],
   "source": [
    "#in nltk, tree is created by giving a node label and a list of children\n",
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "print tree1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP the rabbit)\n"
     ]
    }
   ],
   "source": [
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "print tree2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP chased (NP the rabbit)))\n"
     ]
    }
   ],
   "source": [
    "#combining trees\n",
    "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\n",
    "print tree4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) )\n"
     ]
    }
   ],
   "source": [
    "#Tree Traversal\n",
    "def traverse(t):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        print t,\n",
    "    else:\n",
    "        print '(', t.label(),\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "        print ')', #prevent newline\n",
    "traverse(tree4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Named entity recognition is a task that is well suited to the type of classifier-based approach that we saw for noun phrase chunking. In particular, we can build a tagger that labels each word in a sentence using the IOB format, where chunks are labeled by their appropriate type. Here is part of the CONLL 2002 (conll2002) Dutch training data:\n",
    "\n",
    "Eddy N B-PER  \n",
    "Bonte N I-PER  \n",
    "is V O   \n",
    "woordvoerder N O   \n",
    "van Prep O  \n",
    "diezelfde Pron O   \n",
    "Hogeschool N B-ORG   \n",
    ". Punc O  \n",
    "> In this representation, there is one token per line, each with its part-of-speech tag and its named entity tag. Based on this training corpus, we can construct a tagger that can be used to label new sentences, and use the nltk.chunk.conlltags2tree() function to convert the tag sequences into a chunk tree.\n",
    "\n",
    "> NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the parameter binary=True , then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U.S. is one of the few industrialized nations that *T*-7 does n't have a higher standard of regulation for the smooth , needle-like fibers such as crocidolite that *T*-1 are classified *-5 as amphobiles , according to Brooke T. Mossman , a professor of pathlogy at the University of Vermont College of Medicine .\n",
      "[(u'The', u'DT'), (u'U.S.', u'NNP'), (u'is', u'VBZ'), (u'one', u'CD'), (u'of', u'IN'), (u'the', u'DT'), (u'few', u'JJ'), (u'industrialized', u'VBN'), (u'nations', u'NNS'), (u'that', u'WDT'), (u'*T*-7', u'-NONE-'), (u'does', u'VBZ'), (u\"n't\", u'RB'), (u'have', u'VB'), (u'a', u'DT'), (u'higher', u'JJR'), (u'standard', u'NN'), (u'of', u'IN'), (u'regulation', u'NN'), (u'for', u'IN'), (u'the', u'DT'), (u'smooth', u'JJ'), (u',', u','), (u'needle-like', u'JJ'), (u'fibers', u'NNS'), (u'such', u'JJ'), (u'as', u'IN'), (u'crocidolite', u'NN'), (u'that', u'WDT'), (u'*T*-1', u'-NONE-'), (u'are', u'VBP'), (u'classified', u'VBN'), (u'*-5', u'-NONE-'), (u'as', u'IN'), (u'amphobiles', u'NNS'), (u',', u','), (u'according', u'VBG'), (u'to', u'TO'), (u'Brooke', u'NNP'), (u'T.', u'NNP'), (u'Mossman', u'NNP'), (u',', u','), (u'a', u'DT'), (u'professor', u'NN'), (u'of', u'IN'), (u'pathlogy', u'NN'), (u'at', u'IN'), (u'the', u'DT'), (u'University', u'NNP'), (u'of', u'IN'), (u'Vermont', u'NNP'), (u'College', u'NNP'), (u'of', u'IN'), (u'Medicine', u'NNP'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print ' '.join(nltk.tag.untag(sent))\n",
    "print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (NE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (NE Brooke/NNP)\n",
      "  T./NNP\n",
      "  Mossman/NNP\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NE University/NNP)\n",
      "  of/IN\n",
      "  (NE Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (NE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print nltk.ne_chunk(sent, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not very accurate, `the university of vermont` is not recorgnized correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once named entities have been identified in a text, we then want to extract the relations that exist between them. As indicated earlier, we will typically be looking for relations between specified types of named entity. One way of approaching this task is to initially look for all triples of the form (X, α, Y), where X and Y are named entities of the required types, and α is the string of words that intervenes between X and Y. We can then use regular expressions to pull out just those instances of α that express the relation that we are looking for. The following example searches for strings that contain the word in. The special regular expression (?!\\b.+ing\\b) is a negative lookahead assertion that allows us to disregard strings such as success in supervising the transition of, where in is followed by a gerund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: u'WHYY'] u'in' [LOC: u'Philadelphia']\n",
      "[ORG: u'McGlashan &AMP; Sarrail'] u'firm in' [LOC: u'San Mateo']\n",
      "[ORG: u'Freedom Forum'] u'in' [LOC: u'Arlington']\n",
      "[ORG: u'Brookings Institution'] u', the research group in' [LOC: u'Washington']\n",
      "[ORG: u'Idealab'] u', a self-described business incubator based in' [LOC: u'Los Angeles']\n",
      "[ORG: u'Open Text'] u', based in' [LOC: u'Waterloo']\n",
      "[ORG: u'WGBH'] u'in' [LOC: u'Boston']\n",
      "[ORG: u'Bastille Opera'] u'in' [LOC: u'Paris']\n",
      "[ORG: u'Omnicom'] u'in' [LOC: u'New York']\n",
      "[ORG: u'DDB Needham'] u'in' [LOC: u'New York']\n",
      "[ORG: u'Kaplan Thaler Group'] u'in' [LOC: u'New York']\n",
      "[ORG: u'BBDO South'] u'in' [LOC: u'Atlanta']\n",
      "[ORG: u'Georgia-Pacific'] u'in' [LOC: u'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=IN):\n",
    "        print nltk.sem.rtuple(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we will adopt the formal framework of “generative grammar,” in which a “language” is considered to be nothing more than an enormous collection of all grammatical sentences, and a grammar is a formal notation that can be used for “gen- erating” the members of this set. Grammars use recursive productions of the form S → S and S, as we will explore in Section 8.3. In Chapter 10 we will extend this, to automatically build up the meaning of a sentence out of the meanings of its parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "#context free grammar\n",
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "trees = parser.parse(sent)\n",
    "for tree in trees:\n",
    "    print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we can generate small sentence with n-gram model\n",
    "#but they'll look wired because they're not following the grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coordinate structure**:if v1 and v2 are both phrases of grammatical category X, then __v1 and v2__ is also a phrase of category X.\n",
    "\n",
    "Here are a couple of examples. In the first, two NPs (noun phrases) have been conjoined to make an NP, while in the second, two APs (adjective phrases) have been conjoined to make an AP\n",
    "\n",
    "a. The book’s ending was (NP the worst part and the best part) for me. \n",
    "b. On land they are (AP slow and clumsy looking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**constituent structure**:words combine with other words to form units. The evidence that a sequence of words forms such a unit is given by substitutability—that is, a sequence of words in a well-formed sentence can be replaced by a shorter sequence without rendering the sentence ill-formed.\n",
    "\n",
    "a. The little bear saw the fine fat trout in the brook.  \n",
    "\n",
    "The fact that we can substitute He for The little bear indicates that the latter sequence is a unit. By contrast, we cannot replace little bear saw in the same way.   \n",
    "\n",
    "![](./image/consti.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/syn_cat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Context Free Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"Mary saw Bob\".split()\n",
    "parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in parser.parse(sent):\n",
    "    print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing with CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://www.nltk.org/book/ch08.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
