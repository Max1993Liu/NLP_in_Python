{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#brown corpus\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "print brown.categories()\n",
    "print brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist( (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor'] \n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will'] \n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading own corpus\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = path_file\n",
    "wordLists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordLists.words(wordLists.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bigrams\n",
    "sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n",
    "list(nltk.bigrams(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#proportion of non-stopping words\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return 1.0 * len(content) / len(text)\n",
    "\n",
    "content_fraction(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pronoucing dictionary\n",
    "entries = nltk.corpus.cmudict.entries()\n",
    "for entry in entries[39943:39951]:\n",
    "    print entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find words with similar pronounciation\n",
    "for word, pron in entries:\n",
    "    if len(pron) == 3:\n",
    "        p1, p2, p3 = pron\n",
    "        if p1 == 'P' and p3 == 'T':\n",
    "            print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "#toolbox.entries('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wordnet synonyms\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at concepts that're more specific\n",
    "wn.synset('car.n.01').hyponyms()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at concepts that're more general\n",
    "wn.synset('car.n.01').hypernym_paths()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the `include` relationship\n",
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#semantic similarity of two words\n",
    "right = wn.synset('right_whale.n.01') \n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#higher min_depth, more similar\n",
    "right.lowest_common_hypernyms(minke)[0].min_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "from urllib import urlopen\n",
    "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "raw = urlopen(url).read()\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokens to nltk text\n",
    "text = nltk.Text(tokens)\n",
    "text.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words commonly appear together\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence segmentation\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = nltk.corpus.gutenberg.raw('some_file')\n",
    "sens = sent_tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The text.similar() method takes a word w, finds all contexts w1w w2, then finds all words w' that appear in the same context, i.e. w1w'w2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.similar(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagged corpus\n",
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#what POS happen before a noun\n",
    "word_tag_pairs = nltk.bigrams(brown_news_tagged)\n",
    "list(nltk.FreqDist(a[1] for (a, b) in word_tag_pairs if b[1].startswith('N')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Automatic tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regular expression tagger\n",
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),\n",
    "    (r'.*ed$', 'VBD'),\n",
    "    (r'.*es$', 'VBZ'),\n",
    "    (r'.*ould$', 'MD'),\n",
    "    (r'.*\\'s$', 'NN$'),\n",
    "    (r'.*s$', 'NNS'),\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "    (r'.*', 'NN') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first one that matches is applied\n",
    "regex_tagger = nltk.RegexpTagger(patterns)\n",
    "regex_tagger.tag(brown.sents(categories='news')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look up tagger\n",
    "#use the most frequent words and their most frequent tag\n",
    "fd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "most_freq_words = fd.keys()[:100]\n",
    "likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "\n",
    "sent = brown.sents(categories='news')[3]\n",
    "baseline_tagger.tag(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using backoff\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
    "                                         backoff=nltk.DefaultTagger('NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_tagger.evaluate(brown.tagged_sents(categories='news'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uni-grame tagger\n",
    "#same as look up\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n-gram tagger\n",
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "bigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> N-gram taggers should not consider context that crosses a sentence boundary. Accordingly, NLTK taggers are designed to work with lists of sentences, where each sentence is a list of words. At the start of a sentence, tn-1 and preceding tags are set to None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Taggers   \n",
    "\n",
    "1. Try tagging the token with the bigram tagger.\n",
    "2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger. \n",
    "3. If the unigram tagger is also unable to find a tag, use a default tagger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(brown_tagged_sents, backoff=t0)\n",
    "#cut off specify the pattern has to appear over 2 times to be used\n",
    "t2 = nltk.BigramTagger(brown_tagged_sents, cutoff=2, backoff=t1)\n",
    "\n",
    "t2.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging unknow words  \n",
    "> replacing unknownn words with UNK, so that the n-gram can learn the pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brill Tagging\n",
    "> The general idea is very simple: guess the tag of each word, then go back and fix the mistakes. In this way, a Brill tagger successively transforms a bad tagging of a text into a better one. As with n-gram tagging, this is a supervised learning method, since we need an- notated training data to figure out whether the tagger’s guess is a mistake or not. How- ever, unlike n-gram tagging, it does not count observations but compiles a list of trans- formational correction rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s look at an example in- volving the following sentence:\n",
    "__The President said he will ask Congress to increase grants to states for voca- tional rehabilitation.__\n",
    "> We will examine the operation of two rules: (a) replace NN with VB when the previous word is TO; (b) replace TO with IN when the next tag is NNS. Table 5-6 illustrates this process, first tagging with the unigram tagger, then applying the rules to fix the errors.\n",
    "\n",
    "![Img](./image/Brill.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.tag.brill as brill\n",
    "\n",
    "\"Source code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f219afaf860f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#sentence segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebank_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mboundary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#sentence segmentation\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "\n",
    "boundary = np.array([len(x) for x in sents])-1\n",
    "\n",
    "boundary = np.cumsum(boundary)\n",
    "\n",
    "sents = [x for y in sents for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build up feature set\n",
    "features = []\n",
    "for i in range(1, len(sents)-1):\n",
    "    if sents[i] not in '.?!':\n",
    "        continue\n",
    "    next_word_cap = sents[i+1][0].isupper()\n",
    "    prevword = sents[i-1].lower()\n",
    "    prev_word_one_char = len(sents[i-1]) == 1\n",
    "    is_bountry = i in sents\n",
    "    features.append((next_word_cap, prevword, prev_word_one_char, is_bountry))\n",
    "\n",
    "#then run some classification like naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![information retrieval](./image/info_retrieval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "\n",
    "def ie_process(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Noun Phrase Chunking\n",
    "\n",
    "> In this case, we will define a simple grammar with a single regular expression rule . This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). Using this grammar, we create a chunk parser , and test it on our example sentence . The result is a tree, which we can either print , or display graphically ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.Tree'>\n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar  = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result  = cp.parse(sentence)\n",
    "print type(result)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and nouns\n",
    "{<NNP>+} # chunk sequences of proper nouns \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "(\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IOB tags\n",
    "> In this scheme, each token is tagged with one of three special chunk tags, I (inside), O (outside), or B (begin). A token is tagged as B if it marks the beginning of a chunk. Subsequent tokens within the chunk are tagged I. All other tokens are tagged O.\n",
    "\n",
    "![iob](./image/IOB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing and Evaluating Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [(u'he', u'PRP')]), (u'accepted', u'VBD'), Tree('NP', [(u'the', u'DT')])])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading IOB format and converting to nltk tree\n",
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "'''\n",
    "\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP'])\n",
    "#if come across Lookup Error; Install ghostdriver with:\n",
    "#brew install ghostdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#CoNLL-2000 chunking corpus\n",
    "from nltk.corpus import conll2000\n",
    "print conll2000.chunked_sents('train.txt')[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline: no chunks\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "len(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%\n",
      "    Precision:      0.0%\n",
      "    Recall:         0.0%\n",
      "    F-Measure:      0.0%\n"
     ]
    }
   ],
   "source": [
    "print cp.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%\n",
      "    Precision:     70.6%\n",
      "    Recall:        67.8%\n",
      "    F-Measure:     69.2%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print cp.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using unigram tagger to build chunker\n",
    "#from POS-tag to chunk\n",
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                in zip(sentence, chunktags)]\n",
    "        #note all the tags is in list of tags, so use colltags2tree not collstr2tree\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%\n",
      "    Precision:     79.9%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     83.2%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print unigram_chunker.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                in zip(sentence, chunktags)]\n",
    "        #note all the tags is in list of tags, so use colltags2tree not collstr2tree\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%\n",
      "    Precision:     82.3%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     84.5%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print bigram_chunker.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier-Based Chunkers\n",
    "a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
    "b. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
    "\n",
    "> These two sentences have the same part-of-speech tags, yet they are chunked differ- ently. In the first sentence, the farmer and rice are separate chunks, while the corre- sponding material in the second sentence, the computer monitor, is a single chunk. Clearly, we need to make use of information about the content of the words, in addition to just their part-of-speech tags, if we wish to maximize chunking performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConsecutiveNpChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "            self.classifier = nltk.MaxentClassifier.train(train_set, algorithm='megam', trace=0)\n",
    "            \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classifier(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "\n",
    "class ConsecutiveNpChunk(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
